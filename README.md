# MetaClaw ğŸ¾ v3

![Version](https://img.shields.io/badge/version-3.0.0-blue)
[![Node](https://img.shields.io/badge/node-%3E%3D20-green)](https://nodejs.org)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

**Multi-instance AI assistant platform** with native function calling, Mission Control dashboard, and cross-instance delegation.

---

## ğŸš€ What's New in v3

| Feature | Description |
|---------|-------------|
| **Multi-Instance** | Run multiple AI agents (agent1, agent2, etc.) simultaneously |
| **Mission Control** | Web dashboard for monitoring all instances (uWebSockets.js + vanilla JS) |
| **21 Native Tools** | Shell, web, files, media, scheduling, sub-agents, memory, knowledge |
| **Vision & Voice** | Image analysis (Kimi/Gemini) + voice transcription (Gemini) |
| **Terminal Channel** | CLI/REPL interface for headless servers |
| **WhatsApp Channel** | Baileys integration for WhatsApp Web |
| **Scheduler** | Native cron with Telegram delivery |
| **Session Spawner** | Background AI tasks with auto-announce |
| **Debug Logger** | Per-request/response JSON logs for troubleshooting |
| **Multi-Provider** | Kimi, Claude, Gemini, OpenAI, MiniMax, Ollama support |

---

## âœ¨ Core Features

### Multi-Instance Architecture
- ğŸ—ï¸ **Multiple Agents** â€” Run separate instances with different personalities/purposes
- ğŸ”„ **Cross-Instance Delegation** â€” Delegate tasks between agents via Redis pub/sub
- ğŸ“Š **Centralized Monitoring** â€” Mission Control dashboard for all instances
- ğŸ’¾ **Isolated Storage** â€” Each instance has separate memory, knowledge, and config

### AI Providers
- ğŸŒ™ **Kimi k2.5** â€” Primary model (fast, reliable function calling)
- ğŸ”® **Claude Opus/Sonnet** â€” Complex reasoning tasks
- âœ¨ **Gemini Flash/Pro** â€” Vision and transcription
- ğŸ¤– **OpenAI / MiniMax / Ollama** â€” Fallback and local options

### Channels
- ğŸ’¬ **Telegram** â€” GramJS MTProto integration
- ğŸ“± **WhatsApp** â€” Baileys Web integration (on-demand QR login)
- ğŸ–¥ï¸ **Mission Control** â€” Web dashboard with real-time logs
- âŒ¨ï¸ **Terminal** â€” CLI/REPL for headless servers

### Native Tools (21 Built-in)

| Category | Tools |
|----------|-------|
| **System** | `time`, `shell`, `async_shell` |
| **Files** | `read`, `write`, `ls` |
| **Web** | `search` (Brave), `fetch` |
| **Media** | `image` (vision analysis) |
| **Memory** | `memory_search`, `memory_get` |
| **Knowledge** | `knowledge_search`, `knowledge_add`, `remember` |
| **Agents** | `spawn_subagent`, `active_tasks`, `spawn_kill` |
| **Background** | `bg_run`, `bg_poll`, `bg_list`, `bg_kill` |
| **Scheduling** | `schedule`, `schedule_list`, `schedule_remove` |
| **Communication** | `send_message` |

### Memory & Knowledge
- ğŸ§  **Semantic RAG** â€” Vector search with bge-m3 embeddings
- ğŸ“ **Daily Logs** â€” Auto-save conversations to dated files
- ğŸ” **Memory Search** â€” Semantic + keyword search across all history
- ğŸ“š **Knowledge Base** â€” Persistent facts with tag-based retrieval

### Sub-Agents & Background Tasks
- ğŸ¤– **Autonomous Workers** â€” Spawn AI agents for complex multi-step tasks
- â±ï¸ **Configurable** â€” Max rounds, timeout, model selection per task
- ğŸ”„ **Auto-Retry** â€” Exponential backoff on failures
- ğŸ“ˆ **Progress Tracking** â€” Real-time status updates

### Anti-Duplicate & Safety
- **Active Task Awareness** â€” AI sees running tasks before spawning new ones
- **Spawn Deduplication** â€” Fuzzy matching blocks similar concurrent agents
- **Schedule Deduplication** â€” Same message within 5 min = blocked
- **Emergency Commands** â€” `/stoptasks`, `/stopagents`, `/stopall`
- ğŸ“Š **Progress Reporting** â€” Status updates every 5 turns
- ğŸ’¬ **Communication** â€” Progress reports, mid-task clarification, abort support
- ğŸ”— **Task Chaining** â€” Output of task A feeds into task B

### Session Management
- ğŸ“‘ **Isolated Sessions** â€” Multiple conversation contexts per chat
- ğŸ”€ **Session Switching** â€” Switch between active sessions
- ğŸŒ¿ **Session Branching** â€” Fork with embedding-based context transfer
- ğŸ¤– **AI Compaction** â€” Smart summarization when sessions get long

### Skills (Plugin System)
- ğŸ”Œ **Code-Driven Skills** â€” Register as native function calling tools
- ğŸ¯ **Trigger-Based Loading** â€” Auto-load on matching user queries
- ğŸ“¦ **Install from Git/Local** â€” `installSkill()` from any source
- ğŸ”„ **Hot Reload** â€” Load/unload/reload without restart

### Monitoring
- â¤ï¸ **Heartbeat System** â€” Periodic checks via HEARTBEAT.md (hot-reloadable)
- ğŸ’° **Zero-Token Monitoring** â€” Shell checks first, AI only when conditions trigger
- â° **Smart Scheduler** â€” 3-tier (direct/check/agent) with conditional triggers

### Infrastructure
- ğŸ–¥ï¸ **Multi-Instance Communication** â€” Redis pub/sub, delegate_task between instances
- ğŸ“¨ **Message Queue** â€” Global rate limiting (1.5s), per-chat throttling (3s), flood wait handling
- ğŸ›¡ï¸ **Access Control** â€” Whitelist, auto-reject calls, auto-leave unauthorized groups
- ğŸ“Š **Stats & Cost Tracking** â€” Per-model cost estimates
- ğŸ”„ **Model Fallback** â€” Auto-switch provider on failure

## Requirements
- **OS:** Linux (Ubuntu 22.04+ recommended)
- **Node.js:** v18+
- **npm:** v9+

## Quick Start

```bash
# 1. Clone & install
git clone https://github.com/metasanjaya/metaclaw
cd metaclaw
npm run install-all

# 2. Run the setup wizard
npm run setup

# 3. First-time login (interactive)
node src/gramjs/index.js
# â†’ Enter phone, code, 2FA â†’ wait for "listening" â†’ Ctrl+C

# 4. Start with pm2
pm2 start src/gramjs/index.js --name metaclaw
pm2 save && pm2 startup
```

## Default Model Configuration

```yaml
models:
  # Simple tasks (casual chat, quick answers)
  simple:
    provider: openai
    model: gpt-5.2
    reasoning: medium

  # Complex tasks (analysis, debugging, multi-step)
  complex:
    provider: anthropic
    model: claude-opus-4-6

  # Intent classification & vision
  intent:
    provider: google
    model: gemini-2.5-flash
  vision:
    provider: google
    model: gemini-2.5-flash

  # Fallback
  fallback:
    provider: google
    model: gemini-3

# Sub-Agent models
subagent:
  planner:
    provider: openai
    model: gpt-5.2
    reasoning: high
  executor:
    provider: minimax
    model: MiniMax-M2.5
```

## Commands

| Command | Description |
|---------|-------------|
| `/stats` | Usage statistics |
| `/dailyusage` | Daily stats with cost estimate |
| `/clear` | Delete messages & reset conversation |
| `/remember <text>` | Save to memory |
| `/memory` | Show recent memories |
| `/forget` | Clear today's memory |
| `/subagent <goal>` | Spawn autonomous AI worker |
| `/subagent:status [id]` | Check task status |
| `/subagent:abort <id>` | Abort a running task |
| `/sessions` | List all sessions |
| `/skills` | List installed skills |
| `/heartbeat` | Heartbeat status |
| `/stoptasks` | Stop all async tasks |
| `/stopagents` | Abort all sub-agents |
| `/stopall` | Stop all tasks + agents |
| `/clearall` | Stop + delete all tasks & agents |

## HEARTBEAT.md

```markdown
## interval: 300
## notify: <telegram_user_id>

## Checks
- disk: `df -h / | awk 'NR==2{print $5}' | tr -d %` | if >85 | Disk usage high
- mem: `free -m | awk '/Mem/{printf "%.0f", $3/$2*100}'` | if >90 | Memory high

## Tasks
- email: Check inbox for urgent emails | every 4h
```

## Architecture

```
src/gramjs/
â”œâ”€â”€ GramJSBridge.js        # Main orchestrator
â”œâ”€â”€ GramJSClient.js        # MTProto connection
â”œâ”€â”€ MessageQueue.js        # Rate-limited message sending
â”œâ”€â”€ SubAgent.js            # Autonomous AI workers
â”œâ”€â”€ AsyncTaskManager.js    # Background shell tasks (dedup + rate limit)
â”œâ”€â”€ Scheduler.js           # Persistent job scheduler (dedup)
â”œâ”€â”€ SessionManager.js      # Structured session contexts
â”œâ”€â”€ SkillManager.js        # Plugin system
â”œâ”€â”€ HeartbeatManager.js    # Periodic monitoring
â”œâ”€â”€ ConversationManager.js # Chat history + embeddings
â”œâ”€â”€ KnowledgeManager.js    # Dynamic knowledge base
â”œâ”€â”€ MemoryManager.js       # Memory system
â”œâ”€â”€ RAGEngine.js           # Retrieval-augmented generation
â”œâ”€â”€ InstanceManager.js     # Multi-instance communication
â”œâ”€â”€ StatsTracker.js        # Usage statistics
â””â”€â”€ ChatQueue.js           # Concurrent chat processing

src/ai/
â”œâ”€â”€ UnifiedAIClient.js     # Multi-provider AI client
â””â”€â”€ providers/             # Anthropic, Google, OpenAI, MiniMax
```

## Providers

- **Kimi (Moonshot)** â€” K2.5 (OpenAI-compatible)
- **Anthropic** â€” Claude Opus 4.6, Sonnet 4.5
- **Google** â€” Gemini Flash/Pro/3
- **OpenAI** â€” GPT-5.2, Codex (Responses API)
- **MiniMax** â€” M2.5
- **DeepSeek** â€” DeepSeek Chat
- **Grok (xAI)** â€” Grok-2
- **Z.AI** â€” GLM-5

## Configuration

### Per-Model Temperature
```yaml
models:
  simple:
    provider: kimi
    model: kimi-k2.5
    temperature: 1    # Kimi only accepts 1
```

### Response Delay
```yaml
response_delay:
  dm: 3       # seconds before replying in DM
  group: 5    # seconds before replying in group
```

### Config Validation
Startup validates `config.yaml` against schema (Zod). Invalid configs fail fast with clear error messages.

## License
MIT

---
**MetaClaw** â€” Built by Meta Sanjaya ğŸ¾
